# PANDAS TUTORIALS

print("time take = ", a-b)


'''
####
dataframe.set_index(" column name " ) will set the index as that column name for only that particular instance

use dataframe = dataframe.set_index(" column name ") to change the index for good

another way to go at this is to use -> dataframe.set_index(" column name ", inplace = True)

this does the same thing as above but in one line.


####

btw pandas can plot too -> dataframe['koi col'].plot()

#####

If the file has date in the columns, but they are not *identified* by pandas

use dataframe['date wala col'] = pd.to_datetime(dataframe['date wala col'])

#####

rolling average means -

say we need to find the 25 point rolling average
suppose there are a lot of points of data
we look at one point, and look at the previous 25 points and take the average (hence the rolling average)

can use mean/sum/ etc to get *rolling*

dataframe[' col '].rolling(25).mean().plot() if you want to plot it too...

Use this when your graphs are too jaggedy / rough. this will smooth things out


If the graph still doesnt look pretty, then it probably means the index youre using is not
sorted

if sorting by date, use -> dataframe.sort_index(inplace = true)

now try dataframe[' col '].rolling(25).mean().plot()
#####



Better than col_df = df[df[" index_name_for_that_col"] == "col"]
is using col_df = df.copy()[df["index name for that col"] == "col"]

#####


dataframe[' col ' ].unique presents all the unique values of that col in an array
its similar to using list(set(array_col)) where array_col is a list with repeating elements

#####


doosra_df = df[' col name' ] returns a series (?)

doosra_df = df[[' col name' ]] returns a dataframe

#####

if two dataframes have the same indexes( indices?)
then use df.join(doosra_df[' shared index ka naam '] ) to join the dataframes togther

#####

WHAT ARE F STRINGS ?

graph_df = pd.DataFrame()

for region in df['region'].unique()[:16]:
    print(region)
    region_df = df.copy()[df['region']==region]
    region_df.set_index('Date', inplace=True)
    region_df.sort_index(inplace=True)
    region_df[f"{region}_price25ma"] = region_df["AveragePrice"].rolling(25).mean()

    if graph_df.empty:
        graph_df = region_df[[f"{region}_price25ma"]]  # note the double square brackets!  # F STRINGS USED HERE !!!
    else:
        graph_df = graph_df.join(region_df[f"{region}_price25ma"])


####

df.sort_values(bu= 'col name' , ascending = True, inplace = True) # by default its descending

#####


df.dropna().plot( figsize =( 8, 6) , legend = True/False) # figsize is in inches
#####

df = pd.read_csv("datasets/Minimum Wage Data.csv", encoding="latin")

or

df.to_csv("datasets/minwage.csv", encoding="utf-8")


WHENEVER ENCODING ISSUES OCCUR
'''



'''
import pandas as pd


min_wage = pd.read_csv('minwagedata.csv', encoding="latin")

pd.set_option('display.max_columns', 15)
pd.set_option('display.width', 5000)

print(min_wage.head(10))
print("--------------------------------------------------")

gb = min_wage.groupby("State")
print(gb.get_group("Alaska").set_index("Year").head(10))
print("--------------------------------------------------")

#### to present all the states and their min wage with dates , use group by, its a common method and probably faster than the python logic

actual_min_wage = pd.DataFrame()

for name, group in min_wage.groupby("State"): # or use gb
    if actual_min_wage.empty:
        actual_min_wage = group.set_index("Year")[["Low.2018"]].rename(columns = {"Low.2018": name})
    else:
        actual_min_wage = actual_min_wage.join(group.set_index("Year")[["Low.2018"]].rename(columns = {"Low.2018": name}))

print(actual_min_wage)

print("--------------------------------------------------")

print(actual_min_wage.describe())  # gives us a rundown about basic stats for our dataframe

print("--------------------------------------------------")

print(actual_min_wage.corr())   # prints out the correlations

print("--------------------------------------------------")


#doosra_df = min_wage['Low.2018']==0        ### comparison
doosra_doosra_df = min_wage[min_wage['Low.2018']==0] ###lists states by year with min wage 0


print(doosra_doosra_df.head()) # now that we have a df that has all the state info with no min wage, we can print them out using  doosra_doosra_df.unique()

print("--------------------------------------------------")

import numpy as np

print(actual_min_wage.replace(0, np.NaN).dropna(axis = 1).corr().head())     # removes all the NaN (not a number) values along the y axis/columns and finds the correlation

print("---------------------------------------------------")


min_wage_corr = actual_min_wage.replace(0, np.NaN).dropna(axis = 1).corr()
for problem in doosra_doosra_df['State'].unique():        # to check if any problems exist after dropping Na's
    if problem in min_wage_corr.columns:
        print("Missing something here....")


grouped_issues = doosra_doosra_df.groupby("State") # to get all the problematic states

# we find that these states have 0.0 value for some cols ,
# to check if have *any* cols with values -

for state, data in grouped_issues:
    if data['Low.2018'].sum() != 0.0:
        print("found data for state - ", state)      # supposedly problem solver (?)

print('------------------------------------------------')

##### naya maal


#### matplotlib requires a lot of customization. Need to watch the matplot tutorials to understand better/
import matplotlib.pyplot as plt


##### THE BELOW COMMENTED CODE WORKS FINE, BUT THE LABELS WERE MESSED UP.
'''
'''

labels = [k[:2] for k in min_wage_corr.columns]  # For the labeling- examples - Alaska is al
fig = plt.figure(figsize = (12,12))   # we need the figure to add axis (?)
ax = fig.add_subplot(111) # define the axis, so we can modify. 111 means 1x1 grid, first subplot. if it were 234, then 2x3 grid, fourth subplot
ax.matshow(min_wage_corr, cmap = plt.cm.RdYlGn)
ax.set_xticks(np.arange(len(labels)))
ax.set_yticks(np.arange(len(labels))) # to show the labels

ax.set_yticklabels(labels)# removes the numberings from the axis and puts labels there instead
ax.set_xticklabels(labels)
#plt.show()
'''

'''




### but some of the labels are the same.
# to fix that, we could import a dataset from the internet
# which contains the correct labels


dfs = pd.read_html('https://www.infoplease.com/state-abbreviations-and-state-postal-codes')

for df in dfs:
    print(df.head())

print('---------------------------------------------')

# now this dfs is a list. so ----

state_abbs = dfs[0]  ### 0th index contains the main states

# saving this dataframe

#state_abbs.to_csv('stateAbbs.csv')
# we could do the above , but notice the 0,1,2,3,4 index of the dataframe
# pandas knows what those mean but csv does not and stores them as a meaningful index
# so now the next time you use pandas to import this dataset
# another index of 1234 would be put, essentially making more useless columns
# the correct way to do this would be ---

state_abbs[["State/District", "Postal Code"]].to_csv("stateAbbs.csv", index=False) # the numbering index doesnt mean anything here
# no more faltu index :)

# now

state_abbs = pd.read_csv('stateAbbs.csv', index_col = 0) # 0 means false , by defaults the index col is set to True. THis means the 1234 cols are supposed to be assigned automatically.
# no need to save twice
print(state_abbs)
print('---------------------------------------------')
abbv_dict = state_abbs.to_dict() # convert df to dict, to be able to extract the labels only

###### {key : {key1: value }} can be converted to just {key1 : value} by using dicttt = dicttt['key']

abbv_dict = abbv_dict['Postal Code']
print(abbv_dict)
print('---------------------------------------------')
# DICTS CAN BE TREATED AS DATAFRAMES (?) . Use the same py logic for dicts


## we need to hardcode some troubling abbs
abbv_dict['Federal (FLSA)'] = "FLSA"
abbv_dict['Guam'] = "GU"
abbv_dict['Puerto Rico'] = "PR"

labels = [abbv_dict[k] for k in min_wage_corr.columns] # the .columns part doesnt autofill, doesnt mean what you are doing is wrong. dont depend on autofills
fig = plt.figure(figsize = (12,12))   # we need the figure to add axis (?)
ax = fig.add_subplot(111) # define the axis, so we can modify. 111 means 1x1 grid, first subplot. if it were 234, then 2x3 grid, fourth subplot
ax.matshow(min_wage_corr, cmap = plt.cm.RdYlGn)
ax.set_xticks(np.arange(len(labels)))
ax.set_yticks(np.arange(len(labels))) # to show the labels

ax.set_yticklabels(labels)# removes the numberings from the axis and puts labels there instead
ax.set_xticklabels(labels)

plt.show()


# yay! works fine.
'''

########################################################################################################################

'''import pandas as pd
import numpy as np

pd.set_option('display.max_columns', 100)
pd.set_option('display.width', 100)
unemp = pd.read_csv('output.csv')
print(unemp.head())

df = pd.read_csv('minwagedata.csv', encoding = 'latin')


act_min_wage = pd.DataFrame()

for name, group in df.groupby("State"):
    if act_min_wage.empty:
        act_min_wage = group.set_index("Year")[["Low.2018"]].rename(columns={"Low.2018":name})
    else:
        act_min_wage = act_min_wage.join(group.set_index("Year")[["Low.2018"]].rename(columns={"Low.2018":name}))

act_min_wage = act_min_wage.replace(0, np.NaN).dropna(axis=1)
act_min_wage.head()

def get_min_wage(year, state):
    try:
        return act_min_wage.loc[year][state]
    except:
        return np.NaN
print('---------------------------------')

print(get_min_wage(2012, "Colorado"))

#unemp['min_wage'] = list(map(get_min_wage, unemp['Year'], unemp['State']))

#unemp.to_csv('unempnew.csv', index = False)
unemp = pd.read_csv('unempnew.csv')
print(unemp['Year'])
print('-----------------------------------')

print(unemp[['Rate', 'min_wage']].corr())
print(unemp[['Rate', 'min_wage']].cov())          # the correlation is low but variance is high. interesting.

pres16 = pd.read_csv('pres16results.csv')
top_cands = pres16.head(10)['cand'].values

county_2015 = unemp.copy()[(unemp['Year'] == 2015) & (unemp["Month"]=="February")]     # ERROR ??? it says Year is not a key here.
# to remove state names and replace them with the abbreviationss -

state_abbv = pd.read_csv('stateAbbs.csv', index_col= 0, encoding = 'latin')
state_abbv_dict = state_abbv.to_dict()['Postal Code']

### python magic  ----

county_2015['State'] = county_2015['State'].map(state_abbv_dict)  # all the state names have been replaced by the state abbreviations

 # Pres16 dataframe has a lot more rows than the county2015. so we will have to work accordingly to be able to join them

 ###  now the col names are different in pres16 and county2015 , but they mean the same thing.
 # so we may rename them by

pres16.rename(columns = {"county": "County", "st":"State"}, inplace =True)

for df in [county_2015, pres16]:
    df.set_index(["County", "State"], inplace =True) # important
print('--------------------------------------------------')
pres16 = pres16[pres16['cand'] == 'Donald Trump']
pres16 = pres16[['pct']]
pres16.dropna(inplace = True)
print(pres16.head())  # county and state occur here because we set them as index
print('--------------------------------------------------')
sab_ek_sath = county_2015.merge(pres16, on = ["County", "State"])
sab_ek_sath.dropna(inplace = True)
sab_ek_sath.drop("Year", axis = 1, inplace = True)
print(sab_ek_sath)
print('-----------')

print(sab_ek_sath.corr())
print('--------------')
print(sab_ek_sath.cov())

'''

########################################################################################################################

#DOING ML WITH SCI KIT LEARN AND USING PANDAS

'''import pandas as pd
import numpy as np

df = pd.read_csv('diamonds.csv', index_col = 0)
print(df.head())

# we need to associate each non numerical value with a number. for example in the 'cut' column, we need to assign a number with each type
# as ML is just linear algebra, we need these numbers
# shortcut to do this -
print('-----------------------------------------------')
# # # # #  print(df['cut'].astype('category').cat.codes)

### but this is useless for us rn because it applies arbitrary values to all the types but
# we know that ideal, premium, fair etc are meaningful
# --

print(df['cut'].unique())   # we see here that the number of types are few, so we can hardcode by hand and assign a value to each of them

cut_class_dict = {"Fair": 1, "Good": 2, "Very Good": 3, "Premium": 4, "Ideal": 5}
clarity_dict = {"I3": 1, "I2": 2, "I1": 3, "SI2": 4, "SI1": 5, "VS2": 6, "VS1": 7, "VVS2": 8, "VVS1": 9, "IF": 10, "FL": 11}
color_dict = {"J": 1,"I": 2,"H": 3,"G": 4,"F": 5,"E": 6,"D": 7}

df['cut'] = df['cut'].map(cut_class_dict)
df['clarity'] = df['clarity'].map(clarity_dict)
df['color'] = df['color'].map(color_dict)
print(df.head())     # now all the non numericals have numerical values , which linear algebra likes

from sklearn import svm
import sklearn
from sklearn.linear_model import SGDRegressor
from sklearn.utils import shuffle

df = shuffle(df)

X = df.drop("price", axis = 1).values  # dropped the price on axis 1 , axis 1 means columns
y = df["price"].values # dropped everything but the price

test_size = 200

X_train = X[:-test_size] # up to the last 200
y_train = y[:-test_size]

X_test = X[-test_size:] # the last 200
y_test = y[-test_size:]

#clf = SGDRegressor(max_iter=1000) # some kind of regressor tool   # its absolutely useless
#clf.fit(X_train, y_train)    # fitting the model or training it

#print(clf.score(X_test, y_test))  # we get absolutely bogus values

clf = svm.SVR()

clf.fit(X_train, y_train)
print(clf.score(X_test, y_test)) # Bekar score but okay

for X,y in list(zip(X_test, y_test))[:10]:
    print(f"model predicts {clf.predict([X])[0]}, real value: {y}")'''
